{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Producer, User, Owner in mloda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roles in mloda\n",
    "\n",
    "In this notebook, we will describe three roles which exists in the mloda framework.\n",
    "\n",
    "- Data Producers: Provide access to raw data, business-layer data, or aggregated data.\n",
    "  - A data scientist or analyst might create simpler datasets or analytical outputs\n",
    "  - A data engineer might design access to complex data infrastructures, such as data lakes or warehouses\n",
    "  - Shares plug-ins and with it access to data\n",
    "\n",
    "- Data User: Interacts with mloda by applying plug-ins while making requests via the mlodaAPI.\n",
    "  - A data scientist or analyst who needs data and data transformations (features)\n",
    "  - Consumes features from other parts of the organizations\n",
    "\n",
    "- Data Owner: Ensures lifetime value, availability and governance of data and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Producer\n",
    "\n",
    "Let us look closer into the role of a data producer. \n",
    "\n",
    "What could a data producer create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# We reuse the data from the first example and just rerun it for the sake of the example, but just in one cell.\n\nimport os\nfrom typing import List\nimport mloda\nfrom mloda.user import Feature, DataAccessCollection, PluginLoader\nfrom mloda_plugins.feature_group.input_data.read_dbs.sqlite import SQLITEReader\nfrom mloda_plugins.compute_framework.base_implementations.pyarrow.table import PyArrowTable\n\nplugin_loader = PluginLoader.all()\n\n# Initialize a DataAccessCollection object\ndata_access_collection = DataAccessCollection()\n\n# Define the folders containing the data\n# Note: We use two paths to accommodate different possible root locations as it depends where the code is executed.\nbase_data_path = os.path.join(os.getcwd(), \"docs\", \"docs\", \"examples\", \"mloda_basics\", \"base_data\")\nif not os.path.exists(base_data_path):\n    base_data_path = os.path.join(os.getcwd(), \"base_data\")\n\n# Add the folder to the DataAccessCollection\ndata_access_collection.add_folder(base_data_path)\n\n# As a db cannot work with a folder, we need to add a connection for the db.\ndata_access_collection.add_credential_dict(\n    credential_dict={SQLITEReader.db_path(): os.path.join(base_data_path, \"example.sqlite\")}\n)\n\norder_features: List[str | Feature] = [\"order_id\", \"product_id\", \"quantity\", \"item_price\"]\npayment_features: List[str | Feature] = [\"payment_id\", \"payment_type\", \"payment_status\", \"valid_datetime\"]\nlocation_features: List[str | Feature] = [\"user_location\", \"merchant_location\", \"update_date\"]\ncategorical_features: List[str | Feature] = [\"user_age_group\", \"product_category\", \"transaction_type\"]\nall_features = order_features + payment_features + location_features + categorical_features\n\nmloda.run_all(all_features, data_access_collection=data_access_collection, compute_frameworks={PyArrowTable})"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### FeatureGroup API (plug-in) in short\n\nIn mloda, a data producer defines access by creating feature groups. Here's an example implementation:\n\n```python\nclass FeatureGroupClass(FeatureGroup):\n\n    # Root feature definition\n    @classmethod\n    def input_data(...)\n        ...\n\n    # Features derived from other features\n    def input_features(...)\n        ...\n\n    @classmethod\n    def calculate_feature(...)\n        ...\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Simple example implementation of the FeatureGroup API\n\nIn the background, mloda loads the plug-ins, which were created before, like this one.\n\n```python\nclass ReadFileFeature(FeatureGroup):\n    @classmethod\n    def input_data(cls) -> Optional[BaseInputData]:\n        return ReadFile()\n\n    @classmethod\n    def calculate_feature(cls, data: Any, features: FeatureSet) -> Any:\n        reader = cls.input_data()\n        if reader is not None:\n            data = reader.load(features)\n            return data\n        raise ValueError(f\"Reading file failed for feature {features.get_name_of_one_feature()}.\")\n```\n\nWe use composition to read different data sources. A ReadFile object looks like this:\n\n```python\nclass CsvReader(ReadFile):\n    @classmethod\n    def suffix(cls) -> Tuple[str, ...]:\n        return (\n            \".csv\",\n            \".CSV\",\n        )\n\n    @classmethod\n    def load_data(cls, data_access: Any, features: FeatureSet) -> Any:\n        result = pyarrow_csv.read_csv(data_access)\n        return result.select(list(features.get_all_names()))\n\n    @classmethod\n    def get_column_names(cls, file_name: str) -> Any:\n        read_options = pyarrow_csv.ReadOptions(skip_rows_after_names=1)\n        table = pyarrow_csv.read_csv(file_name, read_options=read_options)\n        return table.schema.names\n```\n\nAs you can see, the implementation is flexible in the sense that if you need something, you can adjust it quite easily. The other files like .json, .parquet and the sqlite access are implemented in a similar fashion."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# In the following, we will just adjust a bit the CsvReader to handle a different delimiter.\n\nfrom typing import Any, Optional\n\nfrom pyarrow import csv as pyarrow_csv\n\nfrom mloda.provider import FeatureSet, BaseInputData\nfrom mloda_plugins.feature_group.input_data.read_file_feature import ReadFileFeature\nfrom mloda_plugins.feature_group.input_data.read_files.csv import CsvReader\n\n\nclass CsvReader2(CsvReader):\n    # Adjusted CsvReader2 to handle the new delimiter\n    _parse_options = pyarrow_csv.ParseOptions(\n        delimiter=\",\",  # Default delimiter\n        quote_char='\"',  # Handles quoted strings\n        ignore_empty_lines=True,  # Skips empty lines\n    )\n\n    @classmethod\n    def load_data(cls, data_access: Any, features: FeatureSet) -> Any:\n        result = pyarrow_csv.read_csv(data_access, parse_options=cls._parse_options)\n        print(\"We used CsvReader2 to load the data.\")\n        return result.select(list(features.get_all_names()))\n\n\nclass ReadFileFeature2(ReadFileFeature):\n    @classmethod\n    def input_data(cls) -> Optional[BaseInputData]:\n        return CsvReader2()\n\n    @classmethod\n    def validate_output_features(cls, data: Any, features: FeatureSet) -> Optional[bool]:\n        for column_name in features.get_all_names():\n            if column_name in data.column_names:\n                column = data[column_name]\n                if column.null_count == column.length:\n                    return False\n            return True\n        return True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from mloda.user import PluginCollector\n\nresult = mloda.run_all(\n    order_features,\n    data_access_collection=data_access_collection,\n    compute_frameworks={PyArrowTable},\n    plugin_collector=PluginCollector.enabled_feature_groups({ReadFileFeature2}),\n)\n\n# We can see that the data was loaded using the new CsvReader2.\n# However, this is a rather simple use case. In a real-world scenario, we would have more complex data and more complex operations."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex plug-ins\n",
    "\n",
    "These can be quite varied:\n",
    "\n",
    "- aggregate features\n",
    "- entity features\n",
    "- historical features\n",
    "\n",
    "Additionally, one can also write feature groups for:\n",
    "\n",
    "- using feature stores\n",
    "- using orchestrator steps\n",
    "- lazy evaluated functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality\n",
    "\n",
    "The producer must optimize quality, which includes:\n",
    "\n",
    "- defining input and output validators\n",
    "- manage the storage and retrieval of artifacts\n",
    "- implementing software testing \n",
    "\n",
    "An integration test could be done by using mlodaAPI.run_all and custom data.\n",
    "\n",
    "An example of a unit test could look like:\n",
    "```python\n",
    "def test_csv_reader_2(self) -> None: \n",
    "   def test_parse_options_are_customized(self, mock_read_csv):\n",
    "        # Ensure the parse options are as expected\n",
    "        expected_parse_options = pyarrow_csv.ParseOptions(\n",
    "            delimiter=\",\",\n",
    "            quote_char='\"',\n",
    "            ignore_empty_lines=True\n",
    "        )\n",
    "\n",
    "        # Call the method to trigger parse options usage\n",
    "        CsvReader2.load_data(Mock(), Mock(spec=FeatureSet))\n",
    "\n",
    "        # Verify that the _parse_options in CsvReader2 are customized\n",
    "        self.assertEqual(CsvReader2._parse_options, expected_parse_options)\n",
    "```\n",
    "\n",
    "This allows us to apply software engineering practices consistently throughout the entire data workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consequences\n",
    "\n",
    "Within mloda, the data producer is empowered as the primary driver, owing to the extensive and customizable range of available plugins.\n",
    "\n",
    "Unlike traditional data toolchains, mloda provides data producers with the flexibility to define their specific start and end points. This enables the versatile application of mloda across different parts of machine learning lifecycle, such as prototyping, training data preparation, or real-time result monitoring.\n",
    "\n",
    "This includes the autonomy to define the boundaries of the data producer's domain and to govern the outflow of data. Both aspects are managed through feature groups, which remain under the direct control of the data producer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data User Role\n\nThe Data User plays a pivotal role in configuring and utilizing the mloda API for machine learning and data workflows. The mloda API offers flexible configurations to cater to diverse use cases across the ML lifecycle. Below is an outline of the configurations and features that define the Data User's role:\n\n```python\nclass mlodaAPI:\n    def __init__(\n        self,\n        requested_features: Union[Features, list[Union[Feature, str]]],\n        compute_frameworks: Union[Set[Type[ComputeFramework]], Optional[list[str]]] = None,\n        links: Optional[Set[Link]] = None,\n        data_access_collection: Optional[DataAccessCollection] = None,\n        global_filter: Optional[GlobalFilter] = None,\n        api_input_data_collection: Optional[ApiInputDataCollection] = None,\n        plugin_collector: Optional[PluginCollector] = None,\n    ) -> None:\n\ndata = mlodaAPI.run_all(requested_feature,...)\n```\n\nLet's use the API to further explain the Data User role. As shown above, there are several configurations to consider. The key ones are:\n\n- Which features to request and if the compute_frameworks should be limited?\n- How data is linked?\n- What specific access rights and permissions does the user have?\n- How data is refined to meet the requirements of the use case?\n\nWith all the given configurations, the mloda core is designed, whenever feasible, to follow the process: \n\n- First, formulate an optimized execution plan\n- Second, to execute the plan accordingly\n\nWhat the user mostly gains is that the process is repeatable and can be run in most environments, as long as the plug-ins are available and the accesses exist (firewalls, credentials). \n\nThe data user could run mloda API in following scenarios:\n\n- POC notebooks\n- Production code scenarios (model training or realtime prediction)\n- Micro service endpont\n- KPI or QA test data ingestion\n\nWith this, the whole ml lifecycle is represented and plug-ins can be reused in a testable and repeatable way along this cycle."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data Owner Role\n\nData owners typically operate at various levels within an organization.\n\nIt can be the one who produces the data, the business stakeholder responsible for the service, or, in some cases, may not be explicitly defined.\n\nIn mloda, the data owner is the one in control of the governance. However, as to date, this system is not included in this open-source offering, as this platform is reserved for development until the plugin ecosystem has a higher degree of maturity.\n\nWe have the plug-in functionalities to integrate governance and operations systems in place. Two simple examples can be:\n\n#### Using organization wide logging\n\n```python\nclass OtelExtender(Extender):\n    def __init__(self) -> None:\n        if trace is None:\n            return\n\n        # Function to be wrapped by the Extender\n        self.wrapped = {ExtenderHook.FEATURE_GROUP_CALCULATE_FEATURE}\n\n    def wraps(self) -> Set[ExtenderHook]:\n        return self.wrapped\n\n    def __call__(self, func: Any, *args: Any, **kwargs: Any) -> Any:\n        logger.warning(\"OtelExtender\")\n        result = func(*args, **kwargs)\n        return result\n```\n\n#### Logging data size\n\n```python\nclass LogSizeOfData(Extender):\n\n    def wraps(self) -> Set[ExtenderHook]:\n        # Function to be wrapped by the Extender\n        return {ExtenderHook.VALIDATE_INPUT_FEATURE}\n\n    def __call__(self, func: Any, *args: Any, **kwargs: Any) -> Any:\n        result = func(*args, **kwargs)\n        size = sys.getsizeof(result)\n        print(f\"Size: {size}\")\n        return result\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we explored the roles of Data Producers, Data Users, and Data Owners within mloda. We delved into the responsibilities and functionalities associated with each role, highlighting how they contribute to the overall data lifecycle.\n",
    "\n",
    "- Data Producers are responsible for implementing the plugins, ensuring the accuracy of data access processes, and defining relevant configuration options\n",
    "\n",
    "- Data Users create usage configuration and apply the mlodaAPI to receive data\n",
    "\n",
    "- Data Owners, while not fully covered in this open-source offering, they are critical for governance and ensuring the ongoing availability and maintenance of essential plugins\n",
    "\n",
    "Understanding these roles and their interactions, shows how mloda's modular and extensible design is vital in bringing the change to efficient data management practices and processing throughout the machine learning lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "While mloda holds a great potential to become a unified portal, we face a key challenge: its plugin coverage is not comprehensive enough to integrate seamlessly across all available tools and technologies. Therefore, active community contributions are absolutely essential to accelerate both its adoption and its ability to transform data management practices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}